{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functional import seq\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def sentiment_symbol(word, positive_words, negative_words):\n",
    "    word = wnl.lemmatize(word)\n",
    "    if word in positive_words:\n",
    "        return '+'\n",
    "    if word in negative_words:\n",
    "        return '-'\n",
    "    return 'n'\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words, pos_words, neg_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join(\n",
    "                [feature_names[i] + \":{0}\".format(\n",
    "                        sentiment_symbol(feature_names[i], pos_words, neg_words)\n",
    "                    )\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = seq.open('names.txt').map(lambda x: x.split()[0].lower()).to_set()\n",
    "positive_words = seq.open('positive-words.txt', mode='rb')\\\n",
    "    .drop(35)\\\n",
    "    .map(lambda x: x.decode(\"utf-8\").strip())\\\n",
    "    .to_set()\n",
    "negative_words = seq.open('negative-words.txt', mode='rb')\\\n",
    "    .drop(35)\\\n",
    "    .map(lambda x: x.decode(\"utf-8\", 'ignore').strip())\\\n",
    "    .to_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Session = namedtuple('Session', ['username',\n",
    "                                 'link',\n",
    "                                 'is_aggression',\n",
    "                                 'is_bullying',\n",
    "                                 'creation_time',\n",
    "                                 'comments'])\n",
    "\n",
    "Comment = namedtuple('Comment', ['user', 'text', 'time'])\n",
    "\n",
    "def parse_row(row):\n",
    "    raw_d = {}\n",
    "    for i, col in enumerate(header):\n",
    "        raw_d[header[i]] = row[i]\n",
    "    comments = []\n",
    "    for i in range(1, 661):\n",
    "        name = 'column{0}'.format(i)\n",
    "        if raw_d[name] != 'empty':\n",
    "            #print(\"COMMENT\")\n",
    "            #print(raw_d[name])\n",
    "            comment = BeautifulSoup(raw_d[name], 'html.parser')\n",
    "            if not comment.font:\n",
    "                continue\n",
    "            c_user = comment.font.text\n",
    "            c_match = re.search('.+::(.+) \\(created_at:(.+)\\)', comment.text)\n",
    "            if not c_match:\n",
    "                continue\n",
    "            c_text, c_time = c_match.groups()\n",
    "            comments.append(Comment(c_user, c_text, c_time))\n",
    "    is_aggression = raw_d['question1'] == 'aggression'\n",
    "    is_bullying = raw_d['question2'] == 'bullying'\n",
    "    return Session(\n",
    "        raw_d['userName'],\n",
    "        raw_d['videolink'],\n",
    "        is_aggression,\n",
    "        is_bullying,\n",
    "        raw_d['postCreatedTime'],\n",
    "        seq(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data = seq.csv('vine_meta_data.csv')\n",
    "header = csv_data.first()\n",
    "data = csv_data.drop(1).map(parse_row).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_usernames = data.map(lambda s: s.username).to_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_comment(usernames, comment):\n",
    "    text = comment.text\n",
    "    for u in usernames:\n",
    "        text = re.sub(re.escape(u), '', text, flags=re.IGNORECASE)\n",
    "    return Comment(comment.user, text, comment.time)\n",
    "\n",
    "def clean_session(session):\n",
    "    usernames = session.comments.map(lambda c: c.user).to_set()\n",
    "    comments = session.comments.map(lambda c: clean_comment(usernames, c))\n",
    "    return Session(\n",
    "        session.username,\n",
    "        session.link,\n",
    "        session.is_aggression,\n",
    "        session.is_bullying,\n",
    "        session.creation_time,\n",
    "        comments)\n",
    "\n",
    "clean_data = data.map(clean_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bully_documents = clean_data\\\n",
    "    .filter(lambda x: x.is_aggression or x.is_bullying)\\\n",
    "    .map(lambda s: s.comments.map(lambda c: c.text).distinct().make_string(' '))\n",
    "non_bully_documents = clean_data\\\n",
    "    .filter_not(lambda x: x.is_aggression or x.is_bullying)\\\n",
    "    .map(lambda s: s.comments.map(lambda c: c.text).distinct().make_string(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = ENGLISH_STOP_WORDS | names\n",
    "bully_count_vectorizer = CountVectorizer(max_df=.65, min_df=2, stop_words=stop_words, binary=True)\n",
    "bully_counts = bully_count_vectorizer.fit_transform(bully_documents)\n",
    "\n",
    "non_bully_count_vectorizer = CountVectorizer(max_df=.65, min_df=2, stop_words=stop_words, binary=True)\n",
    "non_bully_counts = non_bully_count_vectorizer.fit_transform(non_bully_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "people:n fucking:- know:n did:n bitch:- right:+ look:n say:n got:n stop:n hate:- stupid:- make:n vine:n oh:n\n",
      "Topic #1:\n",
      "bitch:- know:n fucking:- look:n ur:n vine:n got:n damn:- really:n people:n yo:n say:n nigga:n funny:- vines:n\n",
      "\n",
      "Topic #0:\n",
      "lol:n just:n know:n fuck:- oh:n shit:- look:n omg:n vine:n did:n good:+ fucking:- really:n got:n im:n\n",
      "Topic #1:\n",
      "lol:n shit:- just:n lmao:n ass:n damn:- fuck:- omg:n did:n nigga:n know:n vine:n ya:n hell:- got:n\n",
      "\n",
      "-850720.748504\n",
      "Topic #0:\n",
      "good:+ oh:n bitch:- wtf:n lmao:n want:n did:n know:n omg:n say:n tho:n look:n said:n got:n make:n\n",
      "Topic #1:\n",
      "bitch:- fucking:- know:n look:n vine:n got:n people:n ur:n really:n say:n think:n stop:n stupid:- vines:n funny:-\n",
      "Topic #2:\n",
      "know:n people:n right:+ bitch:- fucking:- good:+ ur:n look:n vine:n make:n damn:- say:n suck:- got:n time:n\n",
      "\n",
      "Topic #0:\n",
      "just:n lol:n oh:n omg:n fuck:- shit:- vine:n know:n im:n good:+ yes:n look:n fucking:- wtf:n did:n\n",
      "Topic #1:\n",
      "lol:n just:n shit:- ass:n fuck:- got:n look:n know:n lmao:n did:n nigga:n tho:n hell:- funny:- vine:n\n",
      "Topic #2:\n",
      "omg:n lol:n just:n good:+ really:n vines:n know:n did:n look:n haha:n yes:n vine:n best:+ follow:n app:n\n",
      "\n",
      "-856018.374873\n",
      "Topic #0:\n",
      "look:n know:n bitch:- yo:n got:n lmao:n damn:- really:n fucking:- tho:n im:n ur:n nigga:n follow:n right:+\n",
      "Topic #1:\n",
      "bitch:- fucking:- know:n vine:n ur:n people:n look:n really:n stop:n got:n damn:- shut:n make:n say:n nigga:n\n",
      "Topic #2:\n",
      "fucking:- did:n know:n vine:n people:n look:n good:+ said:n right:+ bitch:- think:n say:n better:+ oh:n really:n\n",
      "Topic #3:\n",
      "people:n know:n got:n right:+ fucking:- bitch:- think:n ll:n look:n did:n say:n tho:n make:n want:n damn:-\n",
      "\n",
      "Topic #0:\n",
      "lol:n just:n shit:- look:n fuck:- know:n ass:n omg:n did:n vine:n damn:- tho:n lmao:n got:n funny:-\n",
      "Topic #1:\n",
      "vine:n lol:n ve:n ya:n yes:n hahaha:n watch:n takip:n looks:n follow:n did:n com:n great:+ job:n ederim:n\n",
      "Topic #2:\n",
      "lol:n just:n omg:n oh:n shit:- im:n fuck:- lmao:n know:n vine:n did:n good:+ edits:n fucking:- look:n\n",
      "Topic #3:\n",
      "just:n know:n lol:n think:n shit:- fuck:- ass:n right:+ look:n did:n yes:n really:n say:n people:n fucking:-\n",
      "\n",
      "-859372.304105\n",
      "2 -850720.748504\n"
     ]
    }
   ],
   "source": [
    "def topic_model(counts, count_vectorizer, n_topics):\n",
    "    lda = LatentDirichletAllocation(n_topics=n_topics, learning_method='batch')\n",
    "    lda.fit(counts)\n",
    "\n",
    "    feature_names = count_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, feature_names, 15, positive_words, negative_words)\n",
    "    return lda.score(counts)\n",
    "\n",
    "max_i = None\n",
    "max_p = None\n",
    "for i in range(2, 5):\n",
    "    bp = topic_model(bully_counts, bully_count_vectorizer, i)\n",
    "    nbp = topic_model(non_bully_counts, non_bully_count_vectorizer, i)\n",
    "    print(bp + nbp)\n",
    "    if max_i is None or max_p is None:\n",
    "        max_i = i\n",
    "        max_p = bp + nbp\n",
    "    if bp + nbp > max_p:\n",
    "        max_i = i\n",
    "        max_p = bp + nbp\n",
    "print(max_i, max_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-761126.463018"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-761126.463018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"test\\n\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
